<!doctype html>
<html lang="en">
    <head>
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&family=IBM+Plex+Serif:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">

        <link
            rel="stylesheet"
            href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css"
            integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g=="
            crossorigin="anonymous"
            referrerpolicy="no-referrer"/>

        <!-- math rendering -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

        <meta charset="UTF-8" />
        <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üó∫Ô∏è</text></svg>">
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link href="/index.css" rel="stylesheet">

        <title>Scale-ALiBi</title>
    </head>
    <body>
        <div class="post-layout">
            <nav>

            </nav>
            <article>
                <h1 class="has-authors">Multi-modal, Multi-scale Representation Learning for Satellite Imagery Analysis Just Needs a Good ALiBi</h1>
                <div class="authors">
                    <a target="_blank" href="//ka.ge">Patrick Kage</a><sup>1</sup> and <a href="#">Pavlos Andreadis</a><sup>1</sup>
                </div>

                <div class="institution">
                    <sup>1</sup><a target="_blank" href="https://web.inf.ed.ac.uk/aiai">Artificial Intelligence and its Applications Institute</a>, The University of Edinburgh
                </div>

                <div class="paperlinks">
                    <a href="https://kage.zip/papers/kage_siw_workshop_2024.pdf" target="_blank" class="paperlink"><i class="fa-solid fa-book"></i> <span>Paper</span></a>
                    <a href="//github.com/pkage/scale-alibi" target="_blank" class="paperlink"><i class="fa-brands fa-github"></i> <span>Source code</span></a>
                </div>
                
                

                <h2>Abstract</h2>

                <p>
                    Vision foundation models have been shown to be effective at processing
                    satellite imagery into representations fit for downstream tasks, however,
                    creating models which operate over multiple spatial resolutions and modes
                    is challenging. This paper presents Scale-ALiBi, a linear bias transformer
                    attention mechanism with a spatial encoding bias to relationships between
                    image patches at different ground sample distance scales. We provide an
                    implementation of Scale-ALiBi over a dataset of aligned high- and
                    low-resolution optical and low-resolution SAR satellite imagery data using
                    a triple-contrastive and reconstructive architecture, show an improvement
                    on the GEO-Bench benchmark, and release the newly curated dataset publicly.
                </p>

                <h2>Datasets</h2>

                <p>
                    Alongside the Scale-ALiBi model, we release the dataset
                    used in training. The dataset is generated by processing
                    Sentinel-1 and Sentinel-2 images, segmenting them into <a href="https://en.wikipedia.org/wiki/Tiled_web_map" target="_blank">XYZ</a>
                    tiles of 256x256 pixel resolution. Sentinel-2‚Äôs true-color
                    images are directly segmented, while Sentinel-1 images
                    undergo scaling and band manipulation to create 8-bit
                    optical-like representations. High-resolution images from
                    NAIP are used to match the same tiles, and the next Y-level
                    down is included as well. Due to the constraints of NAIP,
                    the dataset is geographically limited to the continental
                    U.S. and Puerto Rico, with smaller regions selected for
                    coverage based on diversity and scale. Various dataset
                    sizes are made available, see below for download links.
                </p>
                
                <div class="map-controls">
                    <select name="dataset" id="mapdataset">
                        <option value="" selected disabled hidden>Dataset</option>
                        <option value="full">Full</option>
                        <option value="small">Small</option>
                        <option value="micro">Micro</option>
                    </select>
                    <select name="bands" id="mapbands">
                        <option value="" selected disabled hidden>Samples</option>
                        <option value="radar">Synthetic-aperture Radar (Sentinel-1)</option>
                        <option value="lores">Low-resolution Optical (Sentinel-2)</option>
                        <option value="hires">High-resolution Optical (NAIP)</option>
                    </select>
                    <select name="grid" id="mapgrid">
                        <option value="" selected disabled hidden>Grid</option>
                        <option value="on">Grid on</option>
                        <option value="off">Grid off</option>
                    </select>

                    <button type="button" id="mapshow">Show</button>
                </div>
                
            </article>
            <div class="map-container">
                <div id="map"></div>
                
            </div>
            <article>
                <h2>Samples</h2>

                <p>
                    You probably haven't heard of them edison bulb squid prism quinoa. Narwhal vinyl beard taxidermy. Williamsburg blog meditation, fashion axe scenester seitan tacos woke. Jean shorts aesthetic ennui mixtape synth pinterest. Coloring book lo-fi meh man bun, disrupt yes plz salvia yuccie kale chips sriracha vaporware beard williamsburg. Banh mi dreamcatcher vibecession grailed everyday carry gentrify af, vinyl cred lyft vexillologist kale chips etsy banjo paleo. Bushwick coloring book af fingerstache, VHS tofu Brooklyn skateboard.
                </p>
            </article>
            <footer>
                <span class="colorblock" style="background-color: var(--tomato);"></span>
                <span class="colorblock" style="background-color: var(--spring-green);"></span>
                <span class="colorblock" style="background-color: var(--moonstone);"></span>
                
                <span>Patrick Kage &bull; 08 Oct 2024</span>
            </footer>
            <!--div id="map"></div-->
        </div>
        
        <script type="module" src="/main.js"></script>
    </body>
</html>
